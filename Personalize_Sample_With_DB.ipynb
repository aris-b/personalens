{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Personalize Sample with Database\n",
    "\n",
    "The goal of this notebook is to show how to extract data from a database in a batch process and to inject it into Amazon Personalize in order to generate recommendations. \n",
    "\n",
    "This process will use the Django ORM to query the database as well as Pandas for manipulating the query results into usable CSVs for Amazon Personalize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Personalize Into Boto3\n",
    "\n",
    "Given that Amazon Personalize is a service in preview, the boto3 library and AWS CLI must be updated manually in order to allow calls to be enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-02-01 09:51:32--  https://s3-us-west-2.amazonaws.com/personalize-cli-json-models/personalize.json\n",
      "Resolving s3-us-west-2.amazonaws.com (s3-us-west-2.amazonaws.com)... 52.218.204.24\n",
      "Connecting to s3-us-west-2.amazonaws.com (s3-us-west-2.amazonaws.com)|52.218.204.24|:443... connected.\n",
      "HTTP request sent, awaiting response... 304 Not Modified\n",
      "File ‘personalize.json’ not modified on server. Omitting download.\n",
      "\n",
      "--2019-02-01 09:51:32--  https://s3-us-west-2.amazonaws.com/personalize-cli-json-models/personalize-runtime.json\n",
      "Resolving s3-us-west-2.amazonaws.com (s3-us-west-2.amazonaws.com)... 52.218.204.24\n",
      "Connecting to s3-us-west-2.amazonaws.com (s3-us-west-2.amazonaws.com)|52.218.204.24|:443... connected.\n",
      "HTTP request sent, awaiting response... 304 Not Modified\n",
      "File ‘personalize-runtime.json’ not modified on server. Omitting download.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "!wget -N https://s3-us-west-2.amazonaws.com/personalize-cli-json-models/personalize.json\n",
    "!wget -N https://s3-us-west-2.amazonaws.com/personalize-cli-json-models/personalize-runtime.json\n",
    "!aws configure add-model --service-model file://`pwd`/personalize.json --service-name personalize\n",
    "!aws configure add-model --service-model file://`pwd`/personalize-runtime.json --service-name personalize-runtime\n",
    "\n",
    "personalize = boto3.client(service_name='personalize', endpoint_url='https://personalize.us-east-1.amazonaws.com')\n",
    "personalize_runtime = boto3.client(service_name='personalize-runtime', endpoint_url='https://personalize-runtime.us-east-1.amazonaws.com')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify a Bucket and Data Output Location\n",
    "\n",
    "Next specify the bucket that you will be using for your own data, this is where you will initially upload CSV copies of the data extracted from your database.\n",
    "\n",
    "The filenames should reflect their role in the Personalize process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = \"djangopersonalizedemo\"           # replace with the name of your S3 bucket\n",
    "\n",
    "\n",
    "user_metadata = \"user_metadata.csv\"\n",
    "item_metadata = \"item_metadata.csv\"\n",
    "user_interaction = \"user_interaction.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export, Prepare, and Upload Training Data\n",
    "\n",
    "First in this process we will export the user_metadata items and place them in a CSV.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chris/.virtualenvs/personalens/lib/python3.7/site-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
      "  \"\"\")\n"
     ]
    }
   ],
   "source": [
    "# Imports for Django and Pandas\n",
    "\n",
    "import json\n",
    "import datetime\n",
    "import django\n",
    "django.setup()\n",
    "\n",
    "from movielens.models import User\n",
    "from movielens.models import Item\n",
    "from movielens.models import UserData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare and Upload User Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df = pd.DataFrame(list(User.objects.all().values('user_id', 'age','gender','occupation','zip_code')))\n",
    "pd.set_option('display.max_rows', 5)\n",
    "user_df.columns = ['AGE', 'GENDER', 'OCCUPATION', 'USER_ID', 'ZIP_CODE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df.to_csv(user_metadata, index=False)\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(user_metadata).upload_file(user_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare and Upload Item Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ACTION</th>\n",
       "      <th>ADVENTURE</th>\n",
       "      <th>ANIMATION</th>\n",
       "      <th>CHILDREN</th>\n",
       "      <th>COMEDY</th>\n",
       "      <th>CRIME</th>\n",
       "      <th>DOCUMENTARY</th>\n",
       "      <th>DRAMA</th>\n",
       "      <th>FANTASY</th>\n",
       "      <th>FILM_NOIR</th>\n",
       "      <th>...</th>\n",
       "      <th>MYSTERY</th>\n",
       "      <th>ROMANCE</th>\n",
       "      <th>SCI_FI</th>\n",
       "      <th>THRILLER</th>\n",
       "      <th>WAR</th>\n",
       "      <th>WESTERN</th>\n",
       "      <th>ITEM_ID</th>\n",
       "      <th>MOVIE_TITLE</th>\n",
       "      <th>RELEASE_DATE</th>\n",
       "      <th>UNKNOWN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>01-Jan-1995</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>GoldenEye (1995)</td>\n",
       "      <td>01-Jan-1995</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Four Rooms (1995)</td>\n",
       "      <td>01-Jan-1995</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>Get Shorty (1995)</td>\n",
       "      <td>01-Jan-1995</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>Copycat (1995)</td>\n",
       "      <td>01-Jan-1995</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ACTION  ADVENTURE  ANIMATION  CHILDREN  COMEDY  CRIME  DOCUMENTARY  DRAMA  \\\n",
       "0       0          0          1         1       1      0            0      0   \n",
       "1       1          1          0         0       0      0            0      0   \n",
       "2       0          0          0         0       0      0            0      0   \n",
       "3       1          0          0         0       1      0            0      1   \n",
       "4       0          0          0         0       0      1            0      1   \n",
       "\n",
       "   FANTASY  FILM_NOIR  ...  MYSTERY ROMANCE  SCI_FI  THRILLER  WAR  WESTERN  \\\n",
       "0        0          0  ...        0       0       0         0    0        0   \n",
       "1        0          0  ...        0       0       0         1    0        0   \n",
       "2        0          0  ...        0       0       0         1    0        0   \n",
       "3        0          0  ...        0       0       0         0    0        0   \n",
       "4        0          0  ...        0       0       0         1    0        0   \n",
       "\n",
       "   ITEM_ID        MOVIE_TITLE  RELEASE_DATE  UNKNOWN  \n",
       "0        1   Toy Story (1995)   01-Jan-1995        0  \n",
       "1        2   GoldenEye (1995)   01-Jan-1995        0  \n",
       "2        3  Four Rooms (1995)   01-Jan-1995        0  \n",
       "3        4  Get Shorty (1995)   01-Jan-1995        0  \n",
       "4        5     Copycat (1995)   01-Jan-1995        0  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_df = pd.DataFrame(list(Item.objects.all().values('movie_id', 'movie_title', 'release_date', 'IMDB_URL',\n",
    "          'unknown', 'Action', 'Adventure', 'Animation', 'Children', 'Comedy', 'Crime',\n",
    "          'Documentary', 'Drama', 'Fantasy', 'Film_Noir', 'Horror', 'Musical', 'Mystery',\n",
    "          'Romance', 'Sci_Fi', 'Thriller', 'War', 'Western')))\n",
    "item_df.columns\n",
    "new_columns = []\n",
    "for item in item_df.columns:\n",
    "    new_columns.append(item.upper())\n",
    "item_df.columns = new_columns\n",
    "item_df=item_df.rename(columns = {'MOVIE_ID':'ITEM_ID'})\n",
    "item_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_df.to_csv(item_metadata, index=False)\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(item_metadata).upload_file(item_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare and Upload User Interaction Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ITEM_ID</th>\n",
       "      <th>RATING</th>\n",
       "      <th>TIMESTAMP</th>\n",
       "      <th>USER_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1014</td>\n",
       "      <td>5</td>\n",
       "      <td>879781125</td>\n",
       "      <td>286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>222</td>\n",
       "      <td>5</td>\n",
       "      <td>876042340</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55373</th>\n",
       "      <td>538</td>\n",
       "      <td>4</td>\n",
       "      <td>892685437</td>\n",
       "      <td>676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55374</th>\n",
       "      <td>204</td>\n",
       "      <td>5</td>\n",
       "      <td>879795543</td>\n",
       "      <td>716</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>55375 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ITEM_ID  RATING  TIMESTAMP  USER_ID\n",
       "0         1014       5  879781125      286\n",
       "1          222       5  876042340      200\n",
       "...        ...     ...        ...      ...\n",
       "55373      538       4  892685437      676\n",
       "55374      204       5  879795543      716\n",
       "\n",
       "[55375 rows x 4 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note we are using the Django Queryset functionality to select ratings only >= 3.6\n",
    "interaction_df = pd.DataFrame(list(UserData.objects.filter(rating__gte=3.6)\n",
    "                                   .values('user_id', 'item_id', 'rating', 'timestamp')))\n",
    "interaction_df.columns = ['ITEM_ID', 'RATING', 'TIMESTAMP', 'USER_ID']\n",
    "\n",
    "interaction_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_df.to_csv(user_interaction, index=False)\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(user_interaction).upload_file(user_interaction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Schema\n",
    "\n",
    "The next large step will be creating schemas for all 3 files and then placing them inside the Personalize service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the Interaction Schema First\n",
    "\n",
    "This is required to make Personalize function so we will start with the last data exported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"schemaArn\": \"arn:aws:personalize:us-east-1:059124553121:schema/django-interactions-schemafinal\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"f2e46480-63b8-4d26-9a2e-88172520cf2b\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"date\": \"Fri, 01 Feb 2019 20:34:04 GMT\",\n",
      "      \"x-amzn-requestid\": \"f2e46480-63b8-4d26-9a2e-88172520cf2b\",\n",
      "      \"content-length\": \"97\",\n",
      "      \"connection\": \"keep-alive\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "interactions_schema = {\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"Interactions\",\n",
    "    \"namespace\": \"com.amazonaws.personalize.schema\",\n",
    "    \"fields\": [\n",
    "        {\n",
    "            \"name\": \"ITEM_ID\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"RATING\",\n",
    "            \"type\": \"int\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"TIMESTAMP\",\n",
    "            \"type\": \"long\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"USER_ID\",\n",
    "            \"type\": \"string\"\n",
    "        }\n",
    "    ],\n",
    "    \"version\": \"1.0\"\n",
    "}\n",
    "\n",
    "create_schema_response = personalize.create_schema(\n",
    "    name = \"django-interactions-schemafinal\",\n",
    "    schema = json.dumps(interactions_schema)\n",
    ")\n",
    "\n",
    "schema_arn = create_schema_response['schemaArn']\n",
    "print(json.dumps(create_schema_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Wait for Dataset Group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Dataset Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"datasetGroupArn\": \"arn:aws:personalize:us-east-1:059124553121:dataset-group/django-dataset-group\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"d5a7c6aa-734a-45b6-9078-81ec62b90e45\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"date\": \"Fri, 01 Feb 2019 20:03:15 GMT\",\n",
      "      \"x-amzn-requestid\": \"d5a7c6aa-734a-45b6-9078-81ec62b90e45\",\n",
      "      \"content-length\": \"99\",\n",
      "      \"connection\": \"keep-alive\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "create_dataset_group_response = personalize.create_dataset_group(\n",
    "    name = \"django-dataset-group\"\n",
    ")\n",
    "\n",
    "dataset_group_arn = create_dataset_group_response['datasetGroupArn']\n",
    "print(json.dumps(create_dataset_group_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wait for Dataset Group to Have ACTIVE Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetGroup: CREATE PENDING\n",
      "DatasetGroup: ACTIVE\n"
     ]
    }
   ],
   "source": [
    "status = None\n",
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    describe_dataset_group_response = personalize.describe_dataset_group(\n",
    "        datasetGroupArn = dataset_group_arn\n",
    "    )\n",
    "    status = describe_dataset_group_response[\"datasetGroup\"][\"status\"]\n",
    "    print(\"DatasetGroup: {}\".format(status))\n",
    "    \n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"datasetArn\": \"arn:aws:personalize:us-east-1:059124553121:dataset/django-dataset-group/INTERACTIONS\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"5d2f717c-a3c6-4cb4-b404-3bd982155472\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"date\": \"Fri, 01 Feb 2019 20:04:20 GMT\",\n",
      "      \"x-amzn-requestid\": \"5d2f717c-a3c6-4cb4-b404-3bd982155472\",\n",
      "      \"content-length\": \"101\",\n",
      "      \"connection\": \"keep-alive\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "dataset_type = \"INTERACTIONS\"\n",
    "create_dataset_response = personalize.create_dataset(\n",
    "    datasetType = dataset_type,\n",
    "    datasetGroupArn = dataset_group_arn,\n",
    "    schemaArn = schema_arn\n",
    ")\n",
    "\n",
    "dataset_arn = create_dataset_response['datasetArn']\n",
    "print(json.dumps(create_dataset_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare, Create, and Wait for Dataset Import Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attach policy to S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Id\": \"PersonalizeS3BucketAccessPolicy\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Sid\": \"PersonalizeS3BucketAccessPolicy\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": \"personalize.amazonaws.com\"\n",
    "            },\n",
    "            \"Action\": [\n",
    "                \"s3:GetObject\",\n",
    "                \"s3:ListBucket\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:s3:::{}\".format(bucket),\n",
    "                \"arn:aws:s3:::{}/*\".format(bucket)\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "s3.put_bucket_policy(Bucket=bucket, Policy=json.dumps(policy));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create S3 Read Only Access Role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::059124553121:role/PersonalizeS3RoleDjango\n"
     ]
    }
   ],
   "source": [
    "iam = boto3.client(\"iam\")\n",
    "\n",
    "role_name = \"PersonalizeS3RoleDjango\"\n",
    "assume_role_policy_document = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "          \"Effect\": \"Allow\",\n",
    "          \"Principal\": {\n",
    "            \"Service\": \"personalize.amazonaws.com\"\n",
    "          },\n",
    "          \"Action\": \"sts:AssumeRole\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "create_role_response = iam.create_role(\n",
    "    RoleName = role_name,\n",
    "    AssumeRolePolicyDocument = json.dumps(assume_role_policy_document)\n",
    ");\n",
    "\n",
    "iam.attach_role_policy(\n",
    "    RoleName = role_name,\n",
    "    PolicyArn = \"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\"\n",
    ");\n",
    "\n",
    "role_arn = create_role_response[\"Role\"][\"Arn\"]\n",
    "print(role_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Dataset Import Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"datasetImportJobArn\": \"arn:aws:personalize:us-east-1:059124553121:dataset-import-job/django-dataset-import-job5\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"4f331351-db65-467a-a2a9-47d37d876d73\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"date\": \"Fri, 01 Feb 2019 20:42:48 GMT\",\n",
      "      \"x-amzn-requestid\": \"4f331351-db65-467a-a2a9-47d37d876d73\",\n",
      "      \"content-length\": \"114\",\n",
      "      \"connection\": \"keep-alive\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "create_dataset_import_job_response = personalize.create_dataset_import_job(\n",
    "    jobName = \"django-dataset-import-job5\",\n",
    "    datasetArn = dataset_arn,\n",
    "    dataSource = {\n",
    "        \"dataLocation\": \"s3://{}/{}\".format(bucket, user_interaction)\n",
    "    },\n",
    "    roleArn = role_arn\n",
    ")\n",
    "\n",
    "dataset_import_job_arn = create_dataset_import_job_response['datasetImportJobArn']\n",
    "print(json.dumps(create_dataset_import_job_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wait for Dataset Import Job and Dataset Import Job Run to Have ACTIVE Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetImportJob: CREATE PENDING\n",
      "LatestDatasetImportJobRun: CREATE IN_PROGRESS\n",
      "LatestDatasetImportJobRun: CREATE IN_PROGRESS\n",
      "LatestDatasetImportJobRun: CREATE IN_PROGRESS\n",
      "LatestDatasetImportJobRun: CREATE IN_PROGRESS\n",
      "LatestDatasetImportJobRun: CREATE IN_PROGRESS\n",
      "LatestDatasetImportJobRun: CREATE IN_PROGRESS\n",
      "LatestDatasetImportJobRun: CREATE IN_PROGRESS\n",
      "LatestDatasetImportJobRun: CREATE IN_PROGRESS\n",
      "LatestDatasetImportJobRun: CREATE IN_PROGRESS\n",
      "LatestDatasetImportJobRun: CREATE IN_PROGRESS\n",
      "LatestDatasetImportJobRun: CREATE IN_PROGRESS\n",
      "LatestDatasetImportJobRun: ACTIVE\n"
     ]
    }
   ],
   "source": [
    "status = None\n",
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    describe_dataset_import_job_response = personalize.describe_dataset_import_job(\n",
    "        datasetImportJobArn = dataset_import_job_arn\n",
    "    )\n",
    "    \n",
    "    dataset_import_job = describe_dataset_import_job_response[\"datasetImportJob\"]\n",
    "    if \"latestDatasetImportJobRun\" not in dataset_import_job:\n",
    "        status = dataset_import_job[\"status\"]\n",
    "        print(\"DatasetImportJob: {}\".format(status))\n",
    "    else:\n",
    "        status = dataset_import_job[\"latestDatasetImportJobRun\"][\"status\"]\n",
    "        print(\"LatestDatasetImportJobRun: {}\".format(status))\n",
    "    \n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import User Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"schemaArn\": \"arn:aws:personalize:us-east-1:059124553121:schema/django-users-schema\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"d7685786-91f7-4d38-9cff-56124fa3fe1a\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"date\": \"Fri, 01 Feb 2019 21:02:50 GMT\",\n",
      "      \"x-amzn-requestid\": \"d7685786-91f7-4d38-9cff-56124fa3fe1a\",\n",
      "      \"content-length\": \"85\",\n",
      "      \"connection\": \"keep-alive\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# ['AGE', 'GENDER', 'OCCUPATION', 'USER_ID', 'ZIP_CODE']\n",
    "user_schema = {\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"User\",\n",
    "    \"namespace\": \"com.amazonaws.personalize.schema\",\n",
    "    \"fields\": [\n",
    "        {\n",
    "            \"name\": \"AGE\",\n",
    "            \"type\": \"int\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"GENDER\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"OCCUPATION\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"USER_ID\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"ZIP_CODE\",\n",
    "            \"type\": \"string\"\n",
    "        }\n",
    "    ],\n",
    "    \"version\": \"1.0\"\n",
    "}\n",
    "\n",
    "create_schema_response = personalize.create_schema(\n",
    "    name = \"django-users-schema\",\n",
    "    schema = json.dumps(user_schema)\n",
    ")\n",
    "\n",
    "schema_arn = create_schema_response['schemaArn']\n",
    "print(json.dumps(create_schema_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now given that the dataset group was already created, we will continue to use that but updating the dataset type below, then uploading our data into the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"datasetArn\": \"arn:aws:personalize:us-east-1:059124553121:dataset/django-dataset-group/USERS\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"81733c49-a749-413b-90fe-eebcab78f5f2\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"date\": \"Fri, 01 Feb 2019 21:06:16 GMT\",\n",
      "      \"x-amzn-requestid\": \"81733c49-a749-413b-90fe-eebcab78f5f2\",\n",
      "      \"content-length\": \"94\",\n",
      "      \"connection\": \"keep-alive\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "dataset_type = \"USERS\"\n",
    "create_dataset_response = personalize.create_dataset(\n",
    "    datasetType = dataset_type,\n",
    "    datasetGroupArn = dataset_group_arn,\n",
    "    schemaArn = schema_arn\n",
    ")\n",
    "\n",
    "dataset_arn = create_dataset_response['datasetArn']\n",
    "print(json.dumps(create_dataset_response, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"datasetImportJobArn\": \"arn:aws:personalize:us-east-1:059124553121:dataset-import-job/django-import-users\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"8b5d613f-48b0-4bb2-8a75-5f0d4e1dbff6\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"date\": \"Fri, 01 Feb 2019 21:07:29 GMT\",\n",
      "      \"x-amzn-requestid\": \"8b5d613f-48b0-4bb2-8a75-5f0d4e1dbff6\",\n",
      "      \"content-length\": \"107\",\n",
      "      \"connection\": \"keep-alive\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "create_dataset_import_job_response = personalize.create_dataset_import_job(\n",
    "    jobName = \"django-import-users\",\n",
    "    datasetArn = dataset_arn,\n",
    "    dataSource = {\n",
    "        \"dataLocation\": \"s3://{}/{}\".format(bucket, user_metadata)\n",
    "    },\n",
    "    roleArn = role_arn\n",
    ")\n",
    "\n",
    "dataset_import_job_arn = create_dataset_import_job_response['datasetImportJobArn']\n",
    "print(json.dumps(create_dataset_import_job_response, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LatestDatasetImportJobRun: CREATE PENDING\n",
      "LatestDatasetImportJobRun: CREATE IN_PROGRESS\n",
      "LatestDatasetImportJobRun: CREATE IN_PROGRESS\n",
      "LatestDatasetImportJobRun: CREATE IN_PROGRESS\n",
      "LatestDatasetImportJobRun: CREATE IN_PROGRESS\n",
      "LatestDatasetImportJobRun: CREATE IN_PROGRESS\n",
      "LatestDatasetImportJobRun: CREATE IN_PROGRESS\n",
      "LatestDatasetImportJobRun: CREATE IN_PROGRESS\n",
      "LatestDatasetImportJobRun: CREATE IN_PROGRESS\n",
      "LatestDatasetImportJobRun: CREATE IN_PROGRESS\n",
      "LatestDatasetImportJobRun: CREATE IN_PROGRESS\n",
      "LatestDatasetImportJobRun: CREATE IN_PROGRESS\n",
      "LatestDatasetImportJobRun: CREATE IN_PROGRESS\n",
      "LatestDatasetImportJobRun: CREATE IN_PROGRESS\n",
      "LatestDatasetImportJobRun: CREATE IN_PROGRESS\n",
      "LatestDatasetImportJobRun: ACTIVE\n"
     ]
    }
   ],
   "source": [
    "status = None\n",
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    describe_dataset_import_job_response = personalize.describe_dataset_import_job(\n",
    "        datasetImportJobArn = dataset_import_job_arn\n",
    "    )\n",
    "    \n",
    "    dataset_import_job = describe_dataset_import_job_response[\"datasetImportJob\"]\n",
    "    if \"latestDatasetImportJobRun\" not in dataset_import_job:\n",
    "        status = dataset_import_job[\"status\"]\n",
    "        print(\"DatasetImportJob: {}\".format(status))\n",
    "    else:\n",
    "        status = dataset_import_job[\"latestDatasetImportJobRun\"][\"status\"]\n",
    "        print(\"LatestDatasetImportJobRun: {}\".format(status))\n",
    "    \n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Item Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"schemaArn\": \"arn:aws:personalize:us-east-1:059124553121:schema/django-items-schema-final\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"c7b4949f-a458-4ce4-9ea2-0840358980ed\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"date\": \"Fri, 01 Feb 2019 22:10:50 GMT\",\n",
      "      \"x-amzn-requestid\": \"c7b4949f-a458-4ce4-9ea2-0840358980ed\",\n",
      "      \"content-length\": \"91\",\n",
      "      \"connection\": \"keep-alive\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "item_schema = {\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"Item\",\n",
    "    \"namespace\": \"com.amazonaws.personalize.schema\",\n",
    "    \"fields\": [\n",
    "        {\n",
    "            \"name\": \"ACTION\",\n",
    "            \"type\": \"int\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"ADVENTURE\",\n",
    "            \"type\": \"int\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"ANIMATION\",\n",
    "            \"type\": \"int\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"CHILDREN\",\n",
    "            \"type\": \"int\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"COMEDY\",\n",
    "            \"type\": \"int\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"CRIME\",\n",
    "            \"type\": \"int\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"DOCUMENTARY\",\n",
    "            \"type\": \"int\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"DRAMA\",\n",
    "            \"type\": \"int\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"FANTASY\",\n",
    "            \"type\": \"int\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"FILM_NOIR\",\n",
    "            \"type\": \"int\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"HORROR\",\n",
    "            \"type\": \"int\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"IMDB_URL\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"MUSICAL\",\n",
    "            \"type\": \"int\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"MYSTERY\",\n",
    "            \"type\": \"int\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"ROMANCE\",\n",
    "            \"type\": \"int\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"SCI_FI\",\n",
    "            \"type\": \"int\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"THRILLER\",\n",
    "            \"type\": \"int\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"WAR\",\n",
    "            \"type\": \"int\"\n",
    "        },\n",
    "        \n",
    "        {\n",
    "            \"name\": \"WESTERN\",\n",
    "            \"type\": \"int\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"ITEM_ID\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"MOVIE_TITLE\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"RELEASE_DATE\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"UNKNOWN\",\n",
    "            \"type\": \"int\"\n",
    "        }\n",
    "    ],\n",
    "    \"version\": \"1.0\"\n",
    "}\n",
    "\n",
    "create_schema_response = personalize.create_schema(\n",
    "    name = \"django-items-schema-final\",\n",
    "    schema = json.dumps(item_schema)\n",
    ")\n",
    "\n",
    "schema_arn = create_schema_response['schemaArn']\n",
    "print(json.dumps(create_schema_response, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResourceAlreadyExistsException",
     "evalue": "An error occurred (ResourceAlreadyExistsException) when calling the CreateDataset operation: Another dataset of type: ITEMS already exists in the provided dataset group",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceAlreadyExistsException\u001b[0m            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-fcd27fa8018b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mdatasetType\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdatasetGroupArn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_group_arn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mschemaArn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mschema_arn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/personalens/lib/python3.7/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    356\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/personalens/lib/python3.7/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    659\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 661\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    662\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceAlreadyExistsException\u001b[0m: An error occurred (ResourceAlreadyExistsException) when calling the CreateDataset operation: Another dataset of type: ITEMS already exists in the provided dataset group"
     ]
    }
   ],
   "source": [
    "dataset_type = \"ITEMS\"\n",
    "create_dataset_response = personalize.create_dataset(\n",
    "    datasetType = dataset_type,\n",
    "    datasetGroupArn = dataset_group_arn,\n",
    "    schemaArn = schema_arn\n",
    ")\n",
    "\n",
    "dataset_arn = create_dataset_response['datasetArn']\n",
    "print(json.dumps(create_dataset_response, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"datasetImportJobArn\": \"arn:aws:personalize:us-east-1:059124553121:dataset-import-job/django-import-items-final\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"d079eec1-4cfc-429c-ac83-cf98ba48a722\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"date\": \"Fri, 01 Feb 2019 22:11:38 GMT\",\n",
      "      \"x-amzn-requestid\": \"d079eec1-4cfc-429c-ac83-cf98ba48a722\",\n",
      "      \"content-length\": \"113\",\n",
      "      \"connection\": \"keep-alive\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "create_dataset_import_job_response = personalize.create_dataset_import_job(\n",
    "    jobName = \"django-import-items-final\",\n",
    "    datasetArn = dataset_arn,\n",
    "    dataSource = {\n",
    "        \"dataLocation\": \"s3://{}/{}\".format(bucket, item_metadata)\n",
    "    },\n",
    "    roleArn = role_arn\n",
    ")\n",
    "\n",
    "dataset_import_job_arn = create_dataset_import_job_response['datasetImportJobArn']\n",
    "print(json.dumps(create_dataset_import_job_response, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetImportJob: CREATE PENDING\n",
      "LatestDatasetImportJobRun: CREATE FAILED\n"
     ]
    }
   ],
   "source": [
    "status = None\n",
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    describe_dataset_import_job_response = personalize.describe_dataset_import_job(\n",
    "        datasetImportJobArn = dataset_import_job_arn\n",
    "    )\n",
    "    \n",
    "    dataset_import_job = describe_dataset_import_job_response[\"datasetImportJob\"]\n",
    "    if \"latestDatasetImportJobRun\" not in dataset_import_job:\n",
    "        status = dataset_import_job[\"status\"]\n",
    "        print(\"DatasetImportJob: {}\".format(status))\n",
    "    else:\n",
    "        status = dataset_import_job[\"latestDatasetImportJobRun\"][\"status\"]\n",
    "        print(\"LatestDatasetImportJobRun: {}\".format(status))\n",
    "    \n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Recipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:personalize:::recipe/awspersonalizehrnnmodel\n"
     ]
    }
   ],
   "source": [
    "recipe_list = [\n",
    "    \"arn:aws:personalize:::recipe/awspersonalizehrnnmodel\",\n",
    "    \"arn:aws:personalize:::recipe/awspersonalizedeepfmmodel\",\n",
    "    \"arn:aws:personalize:::recipe/awspersonalizesimsmodel\",\n",
    "    \"arn:aws:personalize:::recipe/awspersonalizeffnnmodel\",\n",
    "    \"arn:aws:personalize:::recipe/popularity-baseline\"\n",
    "]\n",
    "\n",
    "recipe_arn = recipe_list[0]\n",
    "print(recipe_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Wait for Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"solutionArn\": \"arn:aws:personalize:us-east-1:059124553121:solution/Django-movielens-soln\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"d8476fd4-c10f-41be-92c1-835dd6c3200c\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"date\": \"Fri, 01 Feb 2019 22:45:14 GMT\",\n",
      "      \"x-amzn-requestid\": \"d8476fd4-c10f-41be-92c1-835dd6c3200c\",\n",
      "      \"content-length\": \"91\",\n",
      "      \"connection\": \"keep-alive\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "create_solution_response = personalize.create_solution(\n",
    "    name = \"Django-movielens-soln\",\n",
    "    datasetGroupArn = dataset_group_arn,\n",
    "    recipeArn = recipe_arn,\n",
    "    minProvisionedTPS = 1\n",
    ")\n",
    "\n",
    "solution_arn = create_solution_response['solutionArn']\n",
    "print(json.dumps(create_solution_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wait for Solution to Have ACTIVE Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution: CREATE IN_PROGRESS\n",
      "Solution: CREATE IN_PROGRESS\n",
      "Solution: CREATE IN_PROGRESS\n",
      "Solution: CREATE IN_PROGRESS\n",
      "Solution: CREATE IN_PROGRESS\n",
      "Solution: CREATE IN_PROGRESS\n",
      "Solution: CREATE IN_PROGRESS\n",
      "Solution: CREATE IN_PROGRESS\n",
      "Solution: CREATE IN_PROGRESS\n",
      "Solution: CREATE IN_PROGRESS\n",
      "Solution: CREATE IN_PROGRESS\n",
      "Solution: CREATE IN_PROGRESS\n",
      "Solution: CREATE IN_PROGRESS\n",
      "Solution: CREATE IN_PROGRESS\n",
      "Solution: CREATE IN_PROGRESS\n",
      "Solution: CREATE IN_PROGRESS\n",
      "Solution: CREATE IN_PROGRESS\n",
      "Solution: CREATE IN_PROGRESS\n",
      "Solution: CREATE IN_PROGRESS\n",
      "Solution: CREATE IN_PROGRESS\n",
      "Solution: CREATE IN_PROGRESS\n",
      "Solution: CREATE IN_PROGRESS\n",
      "Solution: CREATE IN_PROGRESS\n",
      "Solution: CREATE IN_PROGRESS\n",
      "Solution: CREATE IN_PROGRESS\n",
      "Solution: CREATE IN_PROGRESS\n",
      "Solution: CREATE IN_PROGRESS\n",
      "Solution: CREATE IN_PROGRESS\n",
      "Solution: CREATE IN_PROGRESS\n",
      "Solution: CREATE IN_PROGRESS\n",
      "Solution: CREATE IN_PROGRESS\n",
      "Solution: CREATE IN_PROGRESS\n",
      "Solution: CREATE IN_PROGRESS\n",
      "Solution: CREATE IN_PROGRESS\n",
      "Solution: CREATE IN_PROGRESS\n",
      "Solution: CREATE IN_PROGRESS\n",
      "Solution: CREATE IN_PROGRESS\n",
      "Solution: CREATE IN_PROGRESS\n",
      "Solution: CREATE IN_PROGRESS\n",
      "Solution: CREATE IN_PROGRESS\n",
      "Solution: ACTIVE\n"
     ]
    }
   ],
   "source": [
    "status = None\n",
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    describe_solution_response = personalize.describe_solution(\n",
    "        solutionArn = solution_arn\n",
    "    )\n",
    "    status = describe_solution_response[\"solution\"][\"status\"]\n",
    "    print(\"Solution: {}\".format(status))\n",
    "    \n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Metrics of Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"metrics\": {\n",
      "    \"arn:aws:personalize:us-east-1:059124553121:model/awspersonalizehrnnmodel-be286b5e\": {\n",
      "      \"_num_evaluation_users\": 91.0,\n",
      "      \"_num_unique_items\": 1448.0,\n",
      "      \"_user_history_length_10_pct_quantile\": 12.0,\n",
      "      \"_user_history_length_50_pct_quantile\": 34.0,\n",
      "      \"_user_history_length_90_pct_quantile\": 140.0,\n",
      "      \"_user_history_length_mean\": 53.18681318681319,\n",
      "      \"coverage\": 0.27624309392265195,\n",
      "      \"mean_reciprocal_rank\": 0.03940127734228371,\n",
      "      \"normalized_discounted_cumulative_gain_at_10\": 0.045510255107105546,\n",
      "      \"normalized_discounted_cumulative_gain_at_25\": 0.07627253696341692,\n",
      "      \"normalized_discounted_cumulative_gain_at_5\": 0.02671073140739992,\n",
      "      \"precision_at_10\": 0.008791208791208791,\n",
      "      \"precision_at_25\": 0.008351648351648353,\n",
      "      \"precision_at_5\": 0.006593406593406594\n",
      "    }\n",
      "  },\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"06f291df-fa41-40e5-91b9-ba42f50d2144\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"date\": \"Fri, 01 Feb 2019 23:26:14 GMT\",\n",
      "      \"x-amzn-requestid\": \"06f291df-fa41-40e5-91b9-ba42f50d2144\",\n",
      "      \"content-length\": \"722\",\n",
      "      \"connection\": \"keep-alive\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "get_metrics_response = personalize.get_metrics(\n",
    "    solutionArn = solution_arn\n",
    ")\n",
    "\n",
    "print(json.dumps(get_metrics_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Wait for Campaign"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create campaign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"campaignArn\": \"arn:aws:personalize:us-east-1:059124553121:campaign/Django-campaign\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"0a0f6c44-b923-44b0-bb4f-7dbc5f48069c\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"date\": \"Fri, 01 Feb 2019 23:26:32 GMT\",\n",
      "      \"x-amzn-requestid\": \"0a0f6c44-b923-44b0-bb4f-7dbc5f48069c\",\n",
      "      \"content-length\": \"85\",\n",
      "      \"connection\": \"keep-alive\"\n",
      "    },\n",
      "    \"RetryAttempts\": 3\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "create_campaign_response = personalize.create_campaign(\n",
    "    name = \"Django-campaign\",\n",
    "    solutionArn = solution_arn,\n",
    "    updateMode = \"MANUAL\"\n",
    ")\n",
    "\n",
    "campaign_arn = create_campaign_response['campaignArn']\n",
    "print(json.dumps(create_campaign_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wait for Campaign to Have ACTIVE Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Campaign: CREATE PENDING\n",
      "Campaign: CREATE IN_PROGRESS\n",
      "Campaign: CREATE IN_PROGRESS\n",
      "Campaign: CREATE IN_PROGRESS\n",
      "Campaign: CREATE IN_PROGRESS\n",
      "Campaign: CREATE IN_PROGRESS\n",
      "Campaign: CREATE IN_PROGRESS\n",
      "Campaign: CREATE IN_PROGRESS\n",
      "Campaign: ACTIVE\n"
     ]
    }
   ],
   "source": [
    "status = None\n",
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    describe_campaign_response = personalize.describe_campaign(\n",
    "        campaignArn = campaign_arn\n",
    "    )\n",
    "    status = describe_campaign_response[\"campaign\"][\"status\"]\n",
    "    print(\"Campaign: {}\".format(status))\n",
    "    \n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select a User and an Item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER: 711\n",
      "ITEM: Silence of the Lambs, The (1991)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ITEM_ID</th>\n",
       "      <th>TITLE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>GoldenEye (1995)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1680</th>\n",
       "      <td>1681</td>\n",
       "      <td>You So Crazy (1994)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1681</th>\n",
       "      <td>1682</td>\n",
       "      <td>Scream of Stone (Schrei aus Stein) (1991)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1682 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ITEM_ID                                      TITLE\n",
       "0           1                           Toy Story (1995)\n",
       "1           2                           GoldenEye (1995)\n",
       "...       ...                                        ...\n",
       "1680     1681                        You So Crazy (1994)\n",
       "1681     1682  Scream of Stone (Schrei aus Stein) (1991)\n",
       "\n",
       "[1682 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#items = pd.read_csv('./ml-100k/u.item', sep='|', usecols=[0,1], header=None)\n",
    "#items.columns = ['ITEM_ID', 'TITLE']\n",
    "\n",
    "item_title = item_df.loc[items['ITEM_ID'] == item_id].values[0][-1]\n",
    "print(\"USER: {}\".format(user_id))\n",
    "print(\"ITEM: {}\".format(item_title))\n",
    "\n",
    "items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call GetRecommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendations: \n",
      "Conspiracy Theory (1997)\n",
      "Fly Away Home (1996)\n",
      "Critical Care (1997)\n",
      "Paradise Lost: The Child Murders at Robin Hood Hills (1996)\n",
      "3 Ninjas: High Noon At Mega Mountain (1998)\n",
      "Devil's Advocate, The (1997)\n",
      "Mrs. Brown (Her Majesty, Mrs. Brown) (1997)\n",
      "Kiss the Girls (1997)\n",
      "Deconstructing Harry (1997)\n",
      "In the Company of Men (1997)\n",
      "Time Tracers (1995)\n",
      "Donnie Brasco (1997)\n",
      "L.A. Confidential (1997)\n",
      "Starship Troopers (1997)\n",
      "Ulee's Gold (1997)\n",
      "Wag the Dog (1997)\n",
      "Ice Storm, The (1997)\n",
      "Midnight in the Garden of Good and Evil (1997)\n",
      "FairyTale: A True Story (1997)\n",
      "Dark City (1998)\n",
      "U Turn (1997)\n",
      "Desperate Measures (1998)\n",
      "Full Monty, The (1997)\n",
      "Murder at 1600 (1997)\n",
      "Afterglow (1997)\n"
     ]
    }
   ],
   "source": [
    "get_recommendations_response = personalize_runtime.get_recommendations(\n",
    "    campaignArn = campaign_arn,\n",
    "    userId = str(155),\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Recommendations: \")\n",
    "item_list = get_recommendations_response['itemList']\n",
    "for item in item_list:\n",
    "    print(item_df.iloc[int(item['itemId'])]['MOVIE_TITLE'])\n",
    "#print(\"Recommendations: {}\".format(json.dumps(title_list, indent=2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
